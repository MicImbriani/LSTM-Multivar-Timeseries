{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Deep Learning2",
      "language": "python",
      "name": "deep_learning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "LSTM_TEST3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V80ejwHT5_vv"
      },
      "source": [
        "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TJDq3Tv5_v7"
      },
      "source": [
        "# prepare data for lstm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjJ8N4xB5_v9"
      },
      "source": [
        "# https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df =  DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(-i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLHE-1Ts5_v-"
      },
      "source": [
        "# load dataset\n",
        "dataset = read_csv('SeoulBikeData_168.csv', encoding= 'unicode_escape', header=0, index_col=0)\n",
        "\n",
        "values = dataset.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xztLdIb5_v-"
      },
      "source": [
        "# encode the categorical variables into integers\n",
        "label_encoder = LabelEncoder()\n",
        "values[:,12] = label_encoder.fit_transform(values[:,12])\n",
        "values[:,10] = label_encoder.fit_transform(values[:,10])\n",
        "values[:,11] = label_encoder.fit_transform(values[:,11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnaVTr5S5_v_"
      },
      "source": [
        "# ensure all data is float\n",
        "values = values.astype('float32')\n",
        "# normalize features\n",
        "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "#scaled = scaler.fit_transform(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM32xHas5_wA"
      },
      "source": [
        "# frame as supervised learning\n",
        "hours_to_consider = 168\n",
        "reframed = series_to_supervised(values, hours_to_consider, 1)\n",
        "\n",
        "# drop columns we don't want to predict\n",
        "columns_to_keep = list()\n",
        "for i in range(337,reframed.shape[1]+1,14):\n",
        "  columns_to_keep.append(i)\n",
        "\n",
        "n_cols = [i for i in range (337,reframed.shape[1]+1)]\n",
        "for i in columns_to_keep:\n",
        "    n_cols.remove(i)\n",
        "n_cols2 = [x - 1 for x in n_cols]\n",
        "print(reframed)\n",
        "reframed = reframed.drop(reframed.columns[[n_cols2]], axis=1, inplace=False)\n",
        "print(reframed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZ1t7D7K9h7"
      },
      "source": [
        "import pandas as pd\n",
        "my_list = pd.DataFrame(reframed)\n",
        "my_list = my_list.columns.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUQgR2Ue5_wB"
      },
      "source": [
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "train_ratio = 0.85\n",
        "n_train_hours = int(365 * hours_to_consider * train_ratio)\n",
        "\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "\n",
        "# split into input and outputs\n",
        "train_X, train_y = train[:, 1:], train[:, 0]\n",
        "test_X, test_y = test[:, 1:], test[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r82ZRX7w_vzp"
      },
      "source": [
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcWBXXCA5_wC"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.metrics import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "from keras.metrics import MeanAbsoluteError\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.layers import Reshape\n",
        "\n",
        "def adj_r2(r2):\n",
        "  n = train_X.shape[0]\n",
        "  p = train_X.shape[2]\n",
        "  return (1-(1-r2)*(n-1)/(n-p-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JHbtIkP3jQa"
      },
      "source": [
        "def model_builder(hp):\n",
        "  model = Sequential()\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(LSTM(hp_units,\n",
        "                 input_shape=(train_X.shape[1], train_X.shape[2]),\n",
        "                 return_sequences=True\n",
        "                 ))\n",
        "  model.add(BatchNormalization(axis=-1, \n",
        "                            momentum=0.99,\n",
        "                            epsilon=0.001,\n",
        "                            center=True,\n",
        "                            scale=True,\n",
        "                            beta_initializer='zeros',\n",
        "                            gamma_initializer='ones',\n",
        "                            moving_mean_initializer='zeros',\n",
        "                            moving_variance_initializer='ones',\n",
        "                            beta_regularizer=None,\n",
        "                            gamma_regularizer=None,\n",
        "                            beta_constraint=None,\n",
        "                            gamma_constraint=None\n",
        "                            ))\n",
        "\n",
        "  # model.add(Reshape((train_X.shape[1], train_X.shape[2]), input_shape=(3,)))\n",
        "\n",
        "  hp_units2 = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(LSTM(hp_units2, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "  model.add(BatchNormalization(axis=-1, \n",
        "                            momentum=0.99,\n",
        "                            epsilon=0.001,\n",
        "                            center=True,\n",
        "                            scale=True,\n",
        "                            beta_initializer='zeros',\n",
        "                            gamma_initializer='ones',\n",
        "                            moving_mean_initializer='zeros',\n",
        "                            moving_variance_initializer='ones',\n",
        "                            beta_regularizer=None,\n",
        "                            gamma_regularizer=None,\n",
        "                            beta_constraint=None,\n",
        "                            gamma_constraint=None))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('lr', values=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
        "  \n",
        "\n",
        "  model.compile(optimizer=SGD(lr=hp_learning_rate),\n",
        "                loss=\"mse\",\n",
        "                metrics=['accuracy', MeanSquaredError(), RootMeanSquaredError(),MeanAbsoluteError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "tuner = Hyperband(model_builder,\n",
        "                     objective='mean_squared_error',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "tuner.search(train_X, train_y, epochs=100, validation_split=0.15, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "Number of units in the first densely-connected layer: {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('lr')}.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOTX2CXz-d6i"
      },
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_X, train_y, epochs=200, validation_split=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TpnwIhCSUeu"
      },
      "source": [
        "root_mean_squared_error = history.history['root_mean_squared_error']\n",
        "best_epoch = val_acc_per_epoch.index(min(root_mean_squared_error)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Vir9zS-sYm"
      },
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(train_X, train_y, epochs=best_epoch, validation_split=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlpeid6K-0UT"
      },
      "source": [
        "# eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "# print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8AD4j1dKhys"
      },
      "source": [
        "# design network\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "# model.add(BatchNormalization(axis=-1, \n",
        "#                             momentum=0.99,\n",
        "#                             epsilon=0.001,\n",
        "#                             center=True,\n",
        "#                             scale=True,\n",
        "#                             beta_initializer='zeros',\n",
        "#                             gamma_initializer='ones',\n",
        "#                             moving_mean_initializer='zeros',\n",
        "#                             moving_variance_initializer='ones',\n",
        "#                             beta_regularizer=None,\n",
        "#                             gamma_regularizer=None,\n",
        "#                             beta_constraint=None,\n",
        "#                             gamma_constraint=None\n",
        "#                             ))\n",
        "# #model.add(Dropout(rate=0.25))\n",
        "# model.add(Dense(1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# space = {\n",
        "#     \"loss\": np.arange(0.1, 1e-6, 0.00001),\n",
        "#     \"validation_split\": np.arange(0.1, 0.2, 0.01),\n",
        "#     \"batch_size\": [64, 128, 256, 512]\n",
        "#     }\n",
        "\n",
        "# search = RandomizedSearchCV(model, space, n_iter=100, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, random_state=1)\n",
        "# results = search.fit(train_X, train_y)\n",
        "\n",
        "# print('Best Score: %s' % result.best_score_)\n",
        "# print('Best Hyperparameters: %s' % result.best_params_)\n",
        "\n",
        "# MAX_TRIALS = 20\n",
        "# EXECUTION_PER_TRIAL = 3\n",
        "\n",
        "# tuner = RandomSearch(\n",
        "#     model,\n",
        "#     objective='mean_squared_error',\n",
        "#     seed=129537,\n",
        "#     max_trials=MAX_TRIALS,\n",
        "#     executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "#     directory='random_search',\n",
        "#     project_name='cifar10'\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model.compile(loss='mae',\n",
        "#               optimizer=SGD(lr=0.001),\n",
        "#               metrics=[\"accuracy\", \n",
        "#                        MeanSquaredError(), \n",
        "#                        RootMeanSquaredError(),\n",
        "#                        MeanAbsoluteError()])\n",
        "\n",
        "# # fit network\n",
        "# history = model.fit(train_X, \n",
        "#                     train_y,\n",
        "#                     validation_split=0.15,\n",
        "#                     epochs=200,\n",
        "#                     batch_size=60,\n",
        "#                     verbose=2,\n",
        "#                     shuffle=False)\n",
        "\n",
        "# # plot history\n",
        "# pyplot.plot(history.history['loss'], label='train')\n",
        "# pyplot.plot(history.history['val_loss'], label='test')\n",
        "# pyplot.legend()\n",
        "# pyplot.show()\n",
        "\n",
        "# # make a prediction\n",
        "# yhat = model.predict(test_X)\n",
        "# test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "# # invert scaling for forecast\n",
        "# oi = np.random.randint(1,101,size=(853,12))\n",
        "# inv_yhat = concatenate((yhat, oi), axis=1)\n",
        "# print(inv_yhat.shape)\n",
        "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "# inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "# # invert scaling for actual\n",
        "# test_y = test_y.reshape((len(test_y), 1))\n",
        "# inv_y = concatenate((test_y, oi), axis=1)\n",
        "# inv_y = scaler.inverse_transform(inv_y)\n",
        "# inv_y = inv_y[:,0]\n",
        "\n",
        "# # calculate R2\n",
        "# r2 = r2_score(inv_y, inv_yhat)\n",
        "# adj_r2 = adj_r2(r2)\n",
        "# print('Test R2: %.3f' % r2)\n",
        "# print('Test ADJ R2: %.3f' % adj_r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIzPfTiaZ4yu"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['root_mean_squared_error'])\n",
        "plt.plot(history.history['val_root_mean_squared_error'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}