{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Deep Learning2",
      "language": "python",
      "name": "deep_learning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "LSTM_Adam 2layers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V80ejwHT5_vv"
      },
      "source": [
        "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TJDq3Tv5_v7"
      },
      "source": [
        "# prepare data for lstm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjJ8N4xB5_v9"
      },
      "source": [
        "# https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df =  DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(-i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLHE-1Ts5_v-"
      },
      "source": [
        "# load dataset\n",
        "dataset = read_csv('SeoulBikeData_168.csv', encoding= 'unicode_escape', header=0, index_col=0)\n",
        "\n",
        "values = dataset.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xztLdIb5_v-"
      },
      "source": [
        "# encode the categorical variables into integers\n",
        "label_encoder = LabelEncoder()\n",
        "values[:,12] = label_encoder.fit_transform(values[:,12])\n",
        "values[:,10] = label_encoder.fit_transform(values[:,10])\n",
        "values[:,11] = label_encoder.fit_transform(values[:,11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnaVTr5S5_v_"
      },
      "source": [
        "# ensure all data is float\n",
        "values = values.astype('float32')\n",
        "# normalize features\n",
        "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "#scaled = scaler.fit_transform(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM32xHas5_wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f27b47-f06a-4da6-96de-6754dc4c80ec"
      },
      "source": [
        "# frame as supervised learning\n",
        "hours_to_consider = 168\n",
        "reframed = series_to_supervised(values, hours_to_consider, 1)\n",
        "\n",
        "# drop columns we don't want to predict\n",
        "columns_to_keep = list()\n",
        "for i in range(337,reframed.shape[1]+1,14):\n",
        "  columns_to_keep.append(i)\n",
        "\n",
        "n_cols = [i for i in range (337,reframed.shape[1]+1)]\n",
        "for i in columns_to_keep:\n",
        "    n_cols.remove(i)\n",
        "n_cols2 = [x - 1 for x in n_cols]\n",
        "print(reframed)\n",
        "reframed = reframed.drop(reframed.columns[[n_cols2]], axis=1, inplace=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      var1(t-168)  var2(t-168)  var3(t-168)  ...  var12(t)  var13(t)  var14(t)\n",
            "0           254.0          0.0         -5.2  ...       1.0       1.0      12.0\n",
            "1           204.0          1.0         -5.5  ...       1.0       1.0      12.0\n",
            "2           173.0          2.0         -6.0  ...       1.0       1.0      12.0\n",
            "3           107.0          3.0         -6.2  ...       1.0       1.0      12.0\n",
            "4            78.0          4.0         -6.0  ...       1.0       1.0      12.0\n",
            "...           ...          ...          ...  ...       ...       ...       ...\n",
            "8755       1003.0         19.0          4.2  ...       1.0       1.0      11.0\n",
            "8756        764.0         20.0          3.4  ...       1.0       1.0      11.0\n",
            "8757        694.0         21.0          2.6  ...       1.0       1.0      11.0\n",
            "8758        712.0         22.0          2.1  ...       1.0       1.0      11.0\n",
            "8759        584.0         23.0          1.9  ...       1.0       1.0      11.0\n",
            "\n",
            "[8760 rows x 2366 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py:4114: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  result = getitem(key)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZ1t7D7K9h7"
      },
      "source": [
        "import pandas as pd\n",
        "my_list = pd.DataFrame(reframed)\n",
        "my_list = my_list.columns.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUQgR2Ue5_wB"
      },
      "source": [
        "# split into train and test sets\n",
        "values = reframed.values\n",
        "train_ratio = 0.85\n",
        "n_train_hours = int(365 * hours_to_consider * train_ratio)\n",
        "\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "\n",
        "# split into input and outputs\n",
        "train_X, train_y = train[:, 1:], train[:, 0]\n",
        "test_X, test_y = test[:, 1:], test[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r82ZRX7w_vzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4870336c-ca25-4c88-bd05-b216f264ea91"
      },
      "source": [
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8760, 1, 480) (8760,) (0, 1, 480) (0,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcWBXXCA5_wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970845c3-566f-4e31-8b00-3b1096cf1551"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "from keras.metrics import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "from keras.metrics import MeanAbsoluteError\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.layers import Reshape\n",
        "\n",
        "def adj_r2(r2):\n",
        "  n = train_X.shape[0]\n",
        "  p = train_X.shape[2]\n",
        "  return (1-(1-r2)*(n-1)/(n-p-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=fd5c7013f8ce25ea85344d001c047e9fd4dbdb9adc0c562e8bb5fbe928fc031d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=2f36ac60a24609022bef1e9e35f67cae55ee912e5c2a65b2fdd51fbc1dcf3f9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JHbtIkP3jQa",
        "outputId": "8010cac6-4109-40f5-b04e-8c69db6d8d24"
      },
      "source": [
        "def model_builder(hp):\n",
        "  model = Sequential()\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
        "  model.add(LSTM(hp_units,\n",
        "                 input_shape=(train_X.shape[1], train_X.shape[2]),\n",
        "                 return_sequences=True\n",
        "                 ))\n",
        "  model.add(BatchNormalization(axis=-1, \n",
        "                            momentum=0.99,\n",
        "                            epsilon=0.001,\n",
        "                            center=True,\n",
        "                            scale=True,\n",
        "                            beta_initializer='zeros',\n",
        "                            gamma_initializer='ones',\n",
        "                            moving_mean_initializer='zeros',\n",
        "                            moving_variance_initializer='ones',\n",
        "                            beta_regularizer=None,\n",
        "                            gamma_regularizer=None,\n",
        "                            beta_constraint=None,\n",
        "                            gamma_constraint=None\n",
        "                            ))\n",
        "\n",
        "  hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
        "  model.add(LSTM(hp_units2, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "  model.add(BatchNormalization(axis=-1, \n",
        "                            momentum=0.99,\n",
        "                            epsilon=0.001,\n",
        "                            center=True,\n",
        "                            scale=True,\n",
        "                            beta_initializer='zeros',\n",
        "                            gamma_initializer='ones',\n",
        "                            moving_mean_initializer='zeros',\n",
        "                            moving_variance_initializer='ones',\n",
        "                            beta_regularizer=None,\n",
        "                            gamma_regularizer=None,\n",
        "                            beta_constraint=None,\n",
        "                            gamma_constraint=None))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('lr', values=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
        "  \n",
        "  model.compile(optimizer=Adam(hp_learning_rate),\n",
        "                loss=\"mse\",\n",
        "                metrics=['accuracy', MeanSquaredError(), RootMeanSquaredError(),MeanAbsoluteError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "tuner = Hyperband(model_builder,\n",
        "                  objective='mean_squared_error',\n",
        "                  max_epochs=100,\n",
        "                  factor=10,\n",
        "                  directory='my_dir',\n",
        "                  hyperband_iterations=5,\n",
        "                  project_name='logss')\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "tuner.search(train_X, train_y, epochs=1000, validation_split=0.15, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "Number of units in the LSTM layers:\n",
        "Layer 1 = {best_hps.get('units1')}, \n",
        "Layer 2 = {best_hps.get('units2')}.\n",
        "\n",
        "Learning rate = {best_hps.get('lr')}.\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 386 Complete [00h 01m 26s]\n",
            "mean_squared_error: 382752.40625\n",
            "\n",
            "Best mean_squared_error So Far: 94387.296875\n",
            "Total elapsed time: 03h 50m 01s\n",
            "\n",
            "Search: Running Trial #387\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units1            |64                |288               \n",
            "units2            |64                |256               \n",
            "lr                |0.1               |0.0001            \n",
            "tuner/epochs      |10                |100               \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |1                 |0                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/10\n",
            "233/233 [==============================] - 6s 10ms/step - loss: 453874.2654 - accuracy: 0.0021 - mean_squared_error: 453874.2654 - root_mean_squared_error: 668.2679 - mean_absolute_error: 481.5808 - val_loss: 273090.7188 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 273090.7188 - val_root_mean_squared_error: 522.5808 - val_mean_absolute_error: 405.7657\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 1s 6ms/step - loss: 309158.8215 - accuracy: 0.0000e+00 - mean_squared_error: 309158.8215 - root_mean_squared_error: 555.9820 - mean_absolute_error: 423.7631 - val_loss: 321601.5312 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 321601.5312 - val_root_mean_squared_error: 567.0992 - val_mean_absolute_error: 431.8594\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 2s 6ms/step - loss: 319091.9399 - accuracy: 0.0000e+00 - mean_squared_error: 319091.9399 - root_mean_squared_error: 564.8350 - mean_absolute_error: 427.0558 - val_loss: 291049.9688 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 291049.9688 - val_root_mean_squared_error: 539.4905 - val_mean_absolute_error: 412.2314\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 1s 6ms/step - loss: 314219.7975 - accuracy: 0.0000e+00 - mean_squared_error: 314219.7975 - root_mean_squared_error: 560.5249 - mean_absolute_error: 428.3223 - val_loss: 1801773.7500 - val_accuracy: 0.0441 - val_mean_squared_error: 1801773.7500 - val_root_mean_squared_error: 1342.3016 - val_mean_absolute_error: 1227.3713\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 1s 6ms/step - loss: 307261.6131 - accuracy: 0.0000e+00 - mean_squared_error: 307261.6131 - root_mean_squared_error: 554.2526 - mean_absolute_error: 424.2522 - val_loss: 411134.1562 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 411134.1562 - val_root_mean_squared_error: 641.1974 - val_mean_absolute_error: 483.4618\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 2s 7ms/step - loss: 309670.3954 - accuracy: 0.0000e+00 - mean_squared_error: 309670.3954 - root_mean_squared_error: 556.3613 - mean_absolute_error: 422.1020 - val_loss: 482515.1562 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 482515.1562 - val_root_mean_squared_error: 694.6331 - val_mean_absolute_error: 538.7936\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 1s 6ms/step - loss: 308457.5540 - accuracy: 0.0000e+00 - mean_squared_error: 308457.5540 - root_mean_squared_error: 555.2302 - mean_absolute_error: 421.3643 - val_loss: 292237.6562 - val_accuracy: 0.0015 - val_mean_squared_error: 292237.6562 - val_root_mean_squared_error: 540.5901 - val_mean_absolute_error: 413.0166\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 2s 6ms/step - loss: 310667.5799 - accuracy: 0.0000e+00 - mean_squared_error: 310667.5799 - root_mean_squared_error: 557.2439 - mean_absolute_error: 426.9947 - val_loss: 678083.7500 - val_accuracy: 0.0426 - val_mean_squared_error: 678083.7500 - val_root_mean_squared_error: 823.4584 - val_mean_absolute_error: 677.1505\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 1s 6ms/step - loss: 323088.7593 - accuracy: 0.0000e+00 - mean_squared_error: 323088.7593 - root_mean_squared_error: 568.2636 - mean_absolute_error: 431.3394 - val_loss: 289599.9062 - val_accuracy: 0.0015 - val_mean_squared_error: 289599.9062 - val_root_mean_squared_error: 538.1449 - val_mean_absolute_error: 417.2199\n",
            "Epoch 10/10\n",
            "122/233 [==============>...............] - ETA: 0s - loss: 319076.4210 - accuracy: 0.0000e+00 - mean_squared_error: 319076.4210 - root_mean_squared_error: 564.5746 - mean_absolute_error: 436.4465"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOTX2CXz-d6i"
      },
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_X, train_y, epochs=1000, validation_split=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TpnwIhCSUeu"
      },
      "source": [
        "root_mean_squared_error = history.history['val_root_mean_squared_error']\n",
        "best_epoch = root_mean_squared_error.index(min(root_mean_squared_error)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Vir9zS-sYm"
      },
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(train_X, train_y, epochs=best_epoch, validation_split=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlpeid6K-0UT"
      },
      "source": [
        "# eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "# print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8AD4j1dKhys"
      },
      "source": [
        "# # design network\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(224, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "# model.add(BatchNormalization(axis=-1, \n",
        "#                             momentum=0.99,\n",
        "#                             epsilon=0.001,\n",
        "#                             center=True,\n",
        "#                             scale=True,\n",
        "#                             beta_initializer='zeros',\n",
        "#                             gamma_initializer='ones',\n",
        "#                             moving_mean_initializer='zeros',\n",
        "#                             moving_variance_initializer='ones',\n",
        "#                             beta_regularizer=None,\n",
        "#                             gamma_regularizer=None,\n",
        "#                             beta_constraint=None,\n",
        "#                             gamma_constraint=None\n",
        "#                             ))\n",
        "# #model.add(Dropout(rate=0.25))\n",
        "# model.add(Dense(1))\n",
        "\n",
        "\n",
        "# model.compile(loss='mae',\n",
        "#               optimizer=SGD(lr=1e-05),\n",
        "#               metrics=[\"accuracy\", \n",
        "#                        MeanSquaredError(), \n",
        "#                        RootMeanSquaredError(),\n",
        "#                        MeanAbsoluteError()])\n",
        "\n",
        "# # fit network\n",
        "# history = model.fit(train_X, \n",
        "#                     train_y,\n",
        "#                     validation_split=0.15,\n",
        "#                     epochs=166,\n",
        "#                     batch_size=60,\n",
        "#                     verbose=2,\n",
        "#                     shuffle=False)\n",
        "\n",
        "# # plot history\n",
        "# pyplot.plot(history.history['loss'], label='train')\n",
        "# pyplot.plot(history.history['val_loss'], label='test')\n",
        "# pyplot.legend()\n",
        "# pyplot.show()\n",
        "\n",
        "# # make a prediction\n",
        "# yhat = model.predict(test_X)\n",
        "# test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "# # invert scaling for forecast\n",
        "# oi = np.random.randint(1,101,size=(853,12))\n",
        "# inv_yhat = concatenate((yhat, oi), axis=1)\n",
        "# print(inv_yhat.shape)\n",
        "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "# inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "# # invert scaling for actual\n",
        "# test_y = test_y.reshape((len(test_y), 1))\n",
        "# inv_y = concatenate((test_y, oi), axis=1)\n",
        "# inv_y = scaler.inverse_transform(inv_y)\n",
        "# inv_y = inv_y[:,0]\n",
        "\n",
        "# # calculate R2\n",
        "# r2 = r2_score(inv_y, inv_yhat)\n",
        "# adj_r2 = adj_r2(r2)\n",
        "# print('Test R2: %.3f' % r2)\n",
        "# print('Test ADJ R2: %.3f' % adj_r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIzPfTiaZ4yu"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['root_mean_squared_error'])\n",
        "plt.plot(history.history['val_root_mean_squared_error'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}